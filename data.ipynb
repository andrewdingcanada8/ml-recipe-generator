{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% save the dataset into a file\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data_cache/datasets/recipes_raw.zip'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_directory = './data_cache'\n",
    "pathlib.Path(data_directory).mkdir(exist_ok=True)\n",
    "\n",
    "data_file = 'recipes_raw.zip'\n",
    "data_url = 'https://storage.googleapis.com/recipe-box/recipes_raw.zip'\n",
    "\n",
    "tf.keras.utils.get_file(\n",
    "    fname=data_file,\n",
    "    origin=data_url,\n",
    "    cache_dir=data_directory,\n",
    "    extract=True,\n",
    "    archive_format='zip'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% load the data from the file\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_from_file():\n",
    "    file_names = [\n",
    "        'recipes_raw_nosource_ar.json',\n",
    "        'recipes_raw_nosource_epi.json',\n",
    "        'recipes_raw_nosource_fn.json',\n",
    "    ]\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = f'{data_directory}/datasets/{file_name}'\n",
    "\n",
    "        with open(file_path) as dataset_file:\n",
    "            json_data = json.load(dataset_file)\n",
    "            json_data_list = list(json_data.values())\n",
    "            dict_keys = [key for key in json_data_list[0]]\n",
    "            dict_keys.sort()\n",
    "            data += json_data_list\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "data_raw = load_data_from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% filter out recipes that don't have what we need\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_incomplete_recipes(recipe):\n",
    "    required_keys = ['title', 'ingredients', 'instructions']\n",
    "\n",
    "    if not recipe:\n",
    "        return False\n",
    "\n",
    "    for required_key in required_keys:\n",
    "        if not recipe[required_key]:\n",
    "            return False\n",
    "\n",
    "        if type(recipe[required_key]) == list and len(recipe[required_key]) == 0:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "data = [recipe for recipe in data_raw if filter_incomplete_recipes(recipe)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% add the stop words and stringify the data\n"
    }
   },
   "outputs": [],
   "source": [
    "TITLE_STOP_WORD = '🆃🅸🆃🅻🅴\\n'\n",
    "INGREDIENTS_STOP_WORD = '🅸🅽🅶🆁🅴🅳🅸🅴🅽🆃🆂\\n\\n'\n",
    "INSTRUCTIONS_STOP_WORD = '🅸🅽🆂🆃🆁🆄🅲🆃🅸🅾🅽🆂\\n\\n'\n",
    "\n",
    "\n",
    "def recipe_to_string(recipe):\n",
    "    # This string is presented as a part of recipes so we need to clean it up.\n",
    "    noise_string = 'ADVERTISEMENT'\n",
    "\n",
    "    title = recipe['title']\n",
    "    ingredients = recipe['ingredients']\n",
    "    instructions = recipe['instructions'].split('\\n')\n",
    "\n",
    "    ingredients_string = ''\n",
    "    for ingredient in ingredients:\n",
    "        ingredient = ingredient.replace(noise_string, '')\n",
    "        if ingredient:\n",
    "            ingredients_string += f'• {ingredient}\\n'\n",
    "\n",
    "    instructions_string = ''\n",
    "    for instruction in instructions:\n",
    "        instruction = instruction.replace(noise_string, '')\n",
    "        if instruction:\n",
    "            instructions_string += f'▪︎ {instruction}\\n'\n",
    "\n",
    "    return f'{TITLE_STOP_WORD}{title}\\n{INGREDIENTS_STOP_WORD}{ingredients_string}{INSTRUCTIONS_STOP_WORD}{instructions_string}'\n",
    "\n",
    "\n",
    "data = [recipe_to_string(recipe) for recipe in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% filter out super long recipes\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_RECIPE_LENGTH = 1500\n",
    "\n",
    "\n",
    "def filter_recipes_by_length(recipe_test):\n",
    "    return len(recipe_test) <= MAX_RECIPE_LENGTH\n",
    "\n",
    "\n",
    "data = [recipe_text for recipe_text in data if filter_recipes_by_length(recipe_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% create the vocan and tokenize\n"
    }
   },
   "outputs": [],
   "source": [
    "STOP_SIGN = '␣'\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    char_level=True,\n",
    "    filters='',\n",
    "    lower=False,\n",
    "    split=''\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts([STOP_SIGN])\n",
    "tokenizer.fit_on_texts(data)\n",
    "tokenizer.get_config()\n",
    "\n",
    "VOCABULARY_SIZE = len(tokenizer.word_counts) + 1\n",
    "\n",
    "data_vectorized = tokenizer.texts_to_sequences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% pad the data\n"
    }
   },
   "outputs": [],
   "source": [
    "data_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    data_vectorized,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    # We use -1 here and +1 in the next step to make sure\n",
    "    # that all recipes will have at least 1 stops sign at the end,\n",
    "    # since each sequence will be shifted and truncated afterwards\n",
    "    # (to generate X and Y sequences).\n",
    "    maxlen=MAX_RECIPE_LENGTH - 1,\n",
    "    value=tokenizer.texts_to_sequences([STOP_SIGN])[0]\n",
    ")\n",
    "\n",
    "data_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    data_padded,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    maxlen=MAX_RECIPE_LENGTH + 1,\n",
    "    value=tokenizer.texts_to_sequences([STOP_SIGN])[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% save the vectorized data into a pickle\n"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('./saved_data')\n",
    "with open('./saved_data/tokenizer.pickle', 'wb+') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('./saved_data/data_padded.pickle', 'wb+') as handle:\n",
    "    pickle.dump(data_padded, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
